# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SjaGjBa_B0-vp7D9giF0ImaKG_QTVn0W

Importing Necessary Dependncies and Packages
"""

# Importing the Necessary Packages and downloading the extensions
!pip install transformers
!pip install SentencePiece

from transformers import pipeline 
from textblob import TextBlob
from textblob import TextBlob
import random
from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM
import re
import nltk
nltk.download('brown')
import nltk
nltk.download('punkt')
import spacy
import json

import nltk
nltk.download('averaged_perceptron_tagger')
import nltk
nltk.download('wordnet')

def get_mca_questions(context: str):
  # Raise an Exception if the context is not a string
  if not isinstance(context, str):
    raise TypeError("Input must be a string")
  # Summarize the Context using a model, i take this model from Hugging Face Library
  summarizer = pipeline("summarization", model="philschmid/bart-large-cnn-samsum")
  # Assigning the context to Conversation Variable
  conversation = context
  summarized = summarizer(conversation)
  summarized = summarized[0]
  summarized = list(summarized.values())
  summarized = str(summarized)
  # Extracting the Keywords from the Summarized Context
  # Create a TextBlob object
  blob = TextBlob(summarized)
  # Extract keywords
  nouns = blob.noun_phrases
  
  # Combine all extracted keywords into a list
  keywords = nouns
  '''Mapping the Keywords with the Sentences where the keywords are present in the sentences,
   Load the English language model in spaCy'''
  nlp = spacy.load('en_core_web_sm')
  # Process the input text with spaCy
  doc = nlp(conversation)
  # Map keywords to sentences
  sentences = {}
  for keyword in keywords:
    for sentence in doc.sents:
      if keyword in sentence.text.lower():
        if keyword in sentences:
          sentences[keyword].append(sentence.text.strip())
        else:
          sentences[keyword] = [sentence.text.strip()]
  # converting the Sentences into required model format              
  sent = {}
  for i in sentences:
     sent[i]=sentences[i][0]
  # Using Roberta Language Model Inorder to Generate multiple choice questions from Dictionary that contains the list of keywords and their Respective Sentences From Hugging Face
  # importing Tokenizer from Roberta
  tokenizer = AutoTokenizer.from_pretrained("roberta-base")
  # Importing Model from Roberta
  model = AutoModelForMaskedLM.from_pretrained("roberta-base")
  fill_mask = pipeline("fill-mask", model=model, tokenizer=tokenizer)
  questions = []
  for keyword in sent:
    sentence = sent[keyword]
    sentence = re.sub(r'[^\w\s]', ' ', sentence) # replace special characters with spaces
    masked_sentences = []
    for s in sentences[keyword]:
      if keyword in s:
        masked_sentence = s.replace(keyword, tokenizer.mask_token)
        masked_sentences.append(masked_sentence)
        results = fill_mask(masked_sentences)
        correct_answer = keyword.capitalize()
        distractors = []
        for result in results:
          if 'token_str' in result:
            choice = result['token_str'].capitalize()
          if choice != correct_answer and choice not in distractors:
            distractors.append(choice)
          if len(distractors) == 3:
            break
          # Adding the correct answers and distractors into the options
          options = [correct_answer] + distractors

          # Shuffling the options
          random.shuffle(options)
          question = {
            'question':sent[keyword].replace(keyword,"__"),
            'options': options,
            }
          questions.append(question)
  return questions
# Call the function and display the generated multiple choice questions 
mcqs = get_mca_questions(context)
# Printing only unique questions generated by the Model
# Create a dictionary to store unique questions with their options
unique_questions = {}

# Loop through all the questions
for q in mcqs:
    # Extract question and options
    question = q['question']
    options = q['options']
    
    # Check if question already exists in the dictionary
    if question in unique_questions:
        # Get the options for the existing question
        existing_options = unique_questions[question]
        
        # If the current question has more options than the existing one, replace it
        if len(options) > len(existing_options):
            unique_questions[question] = options
    else:
        # Add the new question to the dictionary
        unique_questions[question] = options
# Loop through each unique question
for i, (question, options) in enumerate(unique_questions.items()):
    # Print the question number and question
    print("Question {}: {}".format(i+1, question))
    
    # Loop through each option and print it as a multiple choice option
    for j, option in enumerate(options):
        print("({}) {}".format(chr(97+j), option))
    
    # Add a newline after each question
    print("\n")